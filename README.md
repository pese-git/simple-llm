# Simple LLM Framework

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)]()
[![PyTorch 2.0+](https://img.shields.io/badge/PyTorch-2.0+-red.svg)]()

–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è NLP, –≤–∫–ª—é—á–∞—é—â–∏–π:
- üéØ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ BPE
- üìä –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
- üß† –ú–µ—Ö–∞–Ω–∏–∑–º—ã –≤–Ω–∏–º–∞–Ω–∏—è (Single/Multi-Head)

## –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ
- [–ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç](#–±—ã—Å—Ç—Ä—ã–π-—Å—Ç–∞—Ä—Ç)
- [–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞](#–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
- [–ú–æ–¥—É–ª–∏](#–º–æ–¥—É–ª–∏)
- [–ü—Ä–∏–º–µ—Ä—ã](#–ø—Ä–∏–º–µ—Ä—ã)
- [–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](#–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è)
- [–£—Å—Ç–∞–Ω–æ–≤–∫–∞](#—É—Å—Ç–∞–Ω–æ–≤–∫–∞)

## –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
```python
from simple_llm import SimpleBPE, MultiHeadAttention

# 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
bpe = SimpleBPE().fit(text_corpus)
tokens = bpe.encode("–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞")

# 2. –ú–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ
mha = MultiHeadAttention(
    num_heads=8,
    emb_size=256,
    head_size=32
)
output = mha(torch.randn(1, 10, 256))  # [batch, seq_len, emb_size]
```

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
```mermaid
graph TD
    A[–¢–µ–∫—Å—Ç] --> B(Tokenizer)
    B --> C[–¢–æ–∫–µ–Ω—ã]
    C --> D[TokenEmbeddings]
    D --> E[MultiHeadAttention]
    E --> F[–í—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏]
```

## –ú–æ–¥—É–ª–∏
### –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
- `SimpleBPE` - –±–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è BPE
- `OptimizeBPE` - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è

### –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
- `TokenEmbeddings` - –æ–±—É—á–∞–µ–º—ã–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
- `PositionalEmbeddings` - –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ

### Transformer
- `HeadAttention` - –æ–¥–Ω–æ-–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ
- `MultiHeadAttention` - –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (4-16 –≥–æ–ª–æ–≤)

## –ü—Ä–∏–º–µ—Ä—ã
```bash
# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–æ–≤
python -m example.multi_head_attention_example
```

## –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- [–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è](/doc/bpe_algorithm.md)
- [–≠–º–±–µ–¥–¥–∏–Ω–≥–∏](/doc/token_embeddings_ru.md)
- [MultiHeadAttention](/doc/multi_head_attention_ru.md)

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞
```bash
git clone https://github.com/pese-git/simple-llm.git
cd simple-llm
pip install -e .
```

## –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞
```bash
# –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
pytest tests/ -v

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∏–ª—è –∫–æ–¥–∞
flake8 .

# –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
black .
```

# Simple LLM Framework

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)]()
[![PyTorch 2.0+](https://img.shields.io/badge/PyTorch-2.0+-red.svg)]()

## –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

### –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
- `SimpleBPE` - –∞–ª–≥–æ—Ä–∏—Ç–º Byte Pair Encoding
- `OptimizeBPE` - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è

### –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
- `TokenEmbeddings` - –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
- `PositionalEmbeddings` - –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ

### Transformer Layers
- `HeadAttention` - –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è –æ–¥–Ω–æ–π –≥–æ–ª–æ–≤—ã
- `MultiHeadAttention` - –º–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (4-16 –≥–æ–ª–æ–≤)
- `FeedForward` - –¥–≤—É—Ö—Å–ª–æ–π–Ω–∞—è FFN —Å–µ—Ç—å (—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ ‚Üí —Å–∂–∞—Ç–∏–µ)
- `Decoder` - –ø–æ–ª–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä Transformer (Self-Attention + FFN)

## –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

```python
from simple_llm import SimpleBPE, MultiHeadAttention, FeedForward

# 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
bpe = SimpleBPE().fit(text_corpus)
tokens = bpe.encode("–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞")

# 2. –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω
model = nn.Sequential(
    TokenEmbeddings(10000, 256),
    PositionalEmbeddings(256, 512),
    MultiHeadAttention(8, 256, 32),
    FeedForward(256)
)
```

## –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- [–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è](/doc/bpe_algorithm.md)
- [MultiHeadAttention](/doc/multi_head_attention_ru.md)
- [FeedForward](/doc/feed_forward_ru.md)

## –ü—Ä–∏–º–µ—Ä—ã
```bash
# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–æ–≤
python -m example.multi_head_attention_example  # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–Ω–∏–º–∞–Ω–∏—è
python -m example.feed_forward_example         # –ê–Ω–∞–ª–∏–∑ FFN —Å–ª–æ—è
```

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞
```bash
git clone https://github.com/pese-git/simple-llm.git
cd simple-llm
pip install -e .
```

### –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPT
```python
from simple_llm.transformer.gpt import GPT

model = GPT(
    vocab_size=10000,
    max_seq_len=512,
    emb_size=768,
    num_heads=12,
    head_size=64,
    num_layers=6
)

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
output = model.generate(input_tokens, max_new_tokens=50)
```

## üõ† How-To Guide

### 1. –†–∞–±–æ—Ç–∞ —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º
```python
from simple_llm.tokenizer import SimpleBPE

bpe = SimpleBPE().fit(text_corpus)
tokens = bpe.encode("–¢–µ–∫—Å—Ç –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏")
```

### 2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
```python
from simple_llm.transformer import MultiHeadAttention, FeedForward

attention = MultiHeadAttention(num_heads=8, emb_size=512, head_size=64)
ffn = FeedForward(emb_size=512)
```

### 3. –û–±—É—á–µ–Ω–∏–µ GPT
```python
# –ü—Ä–∏–º–µ—Ä —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è
optimizer = torch.optim.Adam(model.parameters())
loss_fn = nn.CrossEntropyLoss()

for batch in dataloader:
    logits = model(batch['input_ids'])
    loss = loss_fn(logits.view(-1, logits.size(-1)), batch['targets'].view(-1))
    loss.backward()
    optimizer.step()
```

## üìã –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç       | –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ           | –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ         |
|----------------|----------------------|----------------------|
| **–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä**   | x86-64               | 8+ —è–¥–µ—Ä              |
| **–ü–∞–º—è—Ç—å**      | 8GB RAM              | 16GB+ RAM            |
| **GPU**         | –ù–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è         | NVIDIA (8GB+ VRAM)   |
| **–û–°**          | Linux/MacOS/Windows  | Linux                |

## üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

- [–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ GPT](/doc/gpt_documentation_ru.md)
- [–ê–ª–≥–æ—Ä–∏—Ç–º BPE](/doc/bpe_algorithm.md)
- [MultiHeadAttention](/doc/multi_head_attention_ru.md)
- [Decoder](/doc/decoder_ru.md)

## üß™ –ü—Ä–∏–º–µ—Ä—ã
```bash
# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–æ–≤
python -m example.example_gpt           # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
python -m example.multi_head_attention  # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–Ω–∏–º–∞–Ω–∏—è
python -m example.decoder_example       # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –¥–µ–∫–æ–¥–µ—Ä–∞
```

## ü§ù –£—á–∞—Å—Ç–∏–µ –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ
PR –∏ issues –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é—Ç—Å—è! –ü–µ—Ä–µ–¥ –≤–Ω–µ—Å–µ–Ω–∏–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–π:
1. –°–æ–∑–¥–∞–π—Ç–µ issue —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º
2. –°–¥–µ–ª–∞–π—Ç–µ fork —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
3. –û—Ç–∫—Ä–æ–π—Ç–µ Pull Request

## üìú –õ–∏—Ü–µ–Ω–∑–∏—è
MIT License. –ü–æ–¥—Ä–æ–±–Ω–µ–µ –≤ [LICENSE](LICENSE).

# PositionalEmbeddings - Позиционные эмбеддинги

## Назначение
Позиционные эмбеддинги добавляют информацию о порядке элементов в последовательности. Критически важны для:
- Transformer-архитектур
- Моделей обработки текста (BERT, GPT)
- Задач с временными рядами

## Алгоритм работы

```mermaid
flowchart TD
    A[Создание слоя] --> B[Запрос длины последовательности]
    B --> C{Длина в допустимых пределах?}
    C -->|Да| D[Генерация индексов 0..seq_len-1]
    D --> E[Получение векторов из embedding-таблицы]
    E --> F[Возврат эмбеддингов]
    C -->|Нет| G[Ошибка IndexError]
```

1. **Инициализация**:
   - Создается таблица размером `max_seq_len x emb_size`
   - Каждая позиция получает уникальный обучаемый вектор

2. **Работа**:
   - Принимает длину последовательности `seq_len`
   - Возвращает тензор формы `[seq_len, emb_size]`

## Пример использования
```python
# Инициализация
pos_emb = PositionalEmbeddings(max_seq_len=512, emb_size=128)

# Получение эмбеддингов для 50 элементов
embeddings = pos_emb(50)  # shape: [50, 128]

# Интеграция в модель
class TransformerBlock(nn.Module):
    def __init__(self):
        self.pos_emb = PositionalEmbeddings(512, 128)
        
    def forward(self, x):
        pos = self.pos_emb(x.size(1))  # Добавляем к токенным эмбеддингам
        return x + pos
```

## Сравнение подходов
| Метод               | Обучаемость | Плюсы                          | Минусы                |
|----------------------|-------------|--------------------------------|-----------------------|
| Обучаемые            | Да          | Гибкость                       | Требует данных        |
| Синусоидальные       | Нет         | Хорошая обобщающая способность | Фиксированный паттерн |

## Оптимальные практики
- Для `max_seq_len` берите с запасом (+20%)
- Размерность делайте равной размерности токенных эмбеддингов
- Для длинных последовательностей комбинируйте с синусоидальными
